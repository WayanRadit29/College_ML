{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339ba2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------------------------------------\n",
    "# # Information Retrieval Evaluation Exercise\n",
    "# # Python version (equivalent to the provided R script)\n",
    "# # ----------------------------------------------------------\n",
    "\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.stats import pearsonr, ttest_rel\n",
    "\n",
    "\n",
    "# #--Run this cell only if you‚Äôre using Google Colab; otherwise, skip it.---\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# # ---------- CONFIGURATION ----------\n",
    "# QRELS_FILE = \"/content/drive/MyDrive/TRECDATA/qrels.trec8.adhoc\"    # path to qrels file\n",
    "# RUNS_FOLDER = \"/content/drive/MyDrive/TRECDATA/\"       # folder containing ~15 system runs\n",
    "# TOP_K = 10\n",
    "\n",
    "\n",
    "# # ---------- STEP 1: LOAD QRELS ----------\n",
    "# def load_qrels(filepath):\n",
    "#     \"\"\"Load TREC qrels file.\"\"\"\n",
    "#     qrels = pd.read_csv(filepath, sep=r\"\\s+\", header=None, names=[\"topic\", \"iter\", \"docid\", \"relevance\"])\n",
    "#     return qrels\n",
    "\n",
    "\n",
    "# qrels = load_qrels(QRELS_FILE)\n",
    "\n",
    "\n",
    "# # ---------- STEP 2: LOAD SYSTEM RUN ----------\n",
    "# def load_run(set20):\n",
    "#     \"\"\"Load one system run in standard TREC 6-column format.\"\"\"\n",
    "#     run = pd.read_csv(set20, sep=r\"\\s+\", header=None,\n",
    "#                       names=[\"topic\", \"Q0\", \"docid\", \"rank\", \"score\", \"system\"])\n",
    "#     return run\n",
    "\n",
    "\n",
    "# # ---------- STEP 3: COMPUTE PRECISION@K ----------\n",
    "# def precision_at_k(run, qrels, k=10):\n",
    "#     \"\"\"Compute Precision@k per topic.\"\"\"\n",
    "#     topics = run[\"topic\"].unique()\n",
    "#     results = []\n",
    "\n",
    "\n",
    "#     for t in topics:\n",
    "#         docs = run[run[\"topic\"] == t].sort_values(\"rank\").head(k)[\"docid\"].tolist()\n",
    "#         relevant_docs = qrels[(qrels[\"topic\"] == t) & (qrels[\"relevance\"] > 0)][\"docid\"].tolist()\n",
    "#         precision = len(set(docs) & set(relevant_docs)) / k\n",
    "#         results.append({\"topic\": t, \"Pk\": precision})\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# # ---------- STEP 4: LOOP THROUGH FILES INSIDE set20 ----------\n",
    "# all_scores = pd.DataFrame()\n",
    "\n",
    "\n",
    "# SET_FOLDER = os.path.join(RUNS_FOLDER, \"set20\")\n",
    "# files = os.listdir(SET_FOLDER)\n",
    "\n",
    "\n",
    "# print(\"Files in set20:\", files)\n",
    "\n",
    "\n",
    "# for f in files:\n",
    "#     run_path = os.path.join(SET_FOLDER, f)\n",
    "\n",
    "\n",
    "#     # skip directories and non-run files\n",
    "#     if os.path.isdir(run_path):\n",
    "#         continue\n",
    "#     if not f.startswith(\"input.\"):\n",
    "#         print(f\"‚ö†Ô∏è Skipping non-run file: {f}\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "#     print(f\"üìÇ Evaluating: {f}\")\n",
    "#     run = load_run(run_path)\n",
    "\n",
    "\n",
    "#     p10_df = precision_at_k(run, qrels, TOP_K)\n",
    "#     p10_df[\"system\"] = f \n",
    "    \n",
    "#     all_scores = pd.concat([all_scores, p10_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# # ---------- STEP 5: COMPUTE AVERAGE P@10 ----------\n",
    "# avg_scores = all_scores.groupby(\"system\")[\"Pk\"].mean().reset_index(name=\"avg_P10\")\n",
    "# avg_scores = avg_scores.sort_values(\"avg_P10\", ascending=False)\n",
    "\n",
    "\n",
    "# print(\"=== Average Precision@10 for set20 ===\")\n",
    "# print(avg_scores.to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7be521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Information Retrieval Evaluation Exercise\n",
    "# Python version (equivalent to the provided R script)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, ttest_rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040f9642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIGURATION ----------\n",
    "QRELS_FILE = \"qrels.trec8.adhoc\"    # path to qrels file\n",
    "RUNS_FOLDER = \"TRECDATA\"       # folder containing ~15 system runs\n",
    "TOP_K = 5\n",
    "\n",
    "\n",
    "# ---------- STEP 1: LOAD QRELS ----------\n",
    "def load_qrels(filepath):\n",
    "    \"\"\"Load TREC qrels file.\"\"\"\n",
    "    qrels = pd.read_csv(filepath, sep=r\"\\s+\", header=None, names=[\"topic\", \"iter\", \"docid\", \"relevance\"])\n",
    "    return qrels\n",
    "\n",
    "\n",
    "qrels = load_qrels(QRELS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0703824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in set20: ['input.acsys8alo.acsys8alo', 'input.apl8n.apl8n', 'input.CL99SDopt1.CL99SDopt1', 'input.fub99tf.fub99tf', 'input.ibmg99c.ibmg99c', 'input.INQ601.INQ601', 'input.kdd8ps16.kdd8ps16', 'input.mds08a5.mds08a5', 'input.nttd8ale.nttd8ale', 'input.ric8tpx.ric8tpx', 'input.surffal2.surffal2', 'input.unc8al42.unc8al42']\n",
      "üìÇ Evaluating: input.acsys8alo.acsys8alo\n",
      "üìÇ Evaluating: input.apl8n.apl8n\n",
      "üìÇ Evaluating: input.CL99SDopt1.CL99SDopt1\n",
      "üìÇ Evaluating: input.fub99tf.fub99tf\n",
      "üìÇ Evaluating: input.ibmg99c.ibmg99c\n",
      "üìÇ Evaluating: input.INQ601.INQ601\n",
      "üìÇ Evaluating: input.kdd8ps16.kdd8ps16\n",
      "üìÇ Evaluating: input.mds08a5.mds08a5\n",
      "üìÇ Evaluating: input.nttd8ale.nttd8ale\n",
      "üìÇ Evaluating: input.ric8tpx.ric8tpx\n",
      "üìÇ Evaluating: input.surffal2.surffal2\n",
      "üìÇ Evaluating: input.unc8al42.unc8al42\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 2: LOAD SYSTEM RUN ----------\n",
    "def load_run(set20):\n",
    "    \"\"\"Load one system run in standard TREC 6-column format.\"\"\"\n",
    "    run = pd.read_csv(set20, sep=r\"\\s+\", header=None,\n",
    "                      names=[\"topic\", \"Q0\", \"docid\", \"rank\", \"score\", \"system\"])\n",
    "    return run\n",
    "\n",
    "\n",
    "# ---------- STEP 3: COMPUTE PRECISION@K ----------\n",
    "def precision_at_k(run, qrels, k=10):\n",
    "    \"\"\"Compute Precision@k per topic.\"\"\"\n",
    "    topics = run[\"topic\"].unique()\n",
    "    results = []\n",
    "\n",
    "\n",
    "    for t in topics:\n",
    "        docs = run[run[\"topic\"] == t].sort_values(\"rank\").head(k)[\"docid\"].tolist()\n",
    "        relevant_docs = qrels[(qrels[\"topic\"] == t) & (qrels[\"relevance\"] > 0)][\"docid\"].tolist()\n",
    "        precision = len(set(docs) & set(relevant_docs)) / k\n",
    "        results.append({\"topic\": t, \"Pk\": precision})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ---------- STEP 4: LOOP THROUGH FILES INSIDE set20 ----------\n",
    "all_scores = pd.DataFrame()\n",
    "\n",
    "\n",
    "SET_FOLDER = os.path.join(RUNS_FOLDER, \"set20\")\n",
    "files = os.listdir(SET_FOLDER)\n",
    "\n",
    "\n",
    "print(\"Files in set20:\", files)\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    run_path = os.path.join(SET_FOLDER, f)\n",
    "\n",
    "\n",
    "    # skip directories and non-run files\n",
    "    if os.path.isdir(run_path):\n",
    "        continue\n",
    "    if not f.startswith(\"input.\"):\n",
    "        print(f\"‚ö†Ô∏è Skipping non-run file: {f}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    print(f\"üìÇ Evaluating: {f}\")\n",
    "    run = load_run(run_path)\n",
    "\n",
    "\n",
    "    p10_df = precision_at_k(run, qrels, TOP_K)\n",
    "    p10_df[\"system\"] = f \n",
    "    \n",
    "    all_scores = pd.concat([all_scores, p10_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cdd60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Average Precision@10 for set20 ===\n",
      "                     system  avg_P10\n",
      "input.CL99SDopt1.CL99SDopt1    0.800\n",
      "      input.fub99tf.fub99tf    0.604\n",
      "  input.acsys8alo.acsys8alo    0.552\n",
      "    input.nttd8ale.nttd8ale    0.544\n",
      "      input.ric8tpx.ric8tpx    0.500\n",
      "          input.apl8n.apl8n    0.480\n",
      "        input.INQ601.INQ601    0.464\n",
      "      input.mds08a5.mds08a5    0.436\n",
      "      input.ibmg99c.ibmg99c    0.344\n",
      "    input.unc8al42.unc8al42    0.328\n",
      "    input.kdd8ps16.kdd8ps16    0.312\n",
      "    input.surffal2.surffal2    0.012\n"
     ]
    }
   ],
   "source": [
    "# ---------- STEP 5: COMPUTE AVERAGE P@10 ----------\n",
    "avg_scores = all_scores.groupby(\"system\")[\"Pk\"].mean().reset_index(name=\"avg_P10\")\n",
    "avg_scores = avg_scores.sort_values(\"avg_P10\", ascending=False)\n",
    "\n",
    "\n",
    "print(\"=== Average Precision@10 for set20 ===\")\n",
    "print(avg_scores.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e9bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
